## 单个网站静态分析

##### 中目标：单个网站静态分析

#### 爬虫功能

- 对整个目标网站或目标网站的一个栏目进行采集。采集内容不超出该网站或栏目
- 正确处理网页编码。
- 可以过滤特定的资源类型。如只抓取htm/html/txt等文本内容，不采集图片，视频等信息
- 可以断点恢复。遇到采集过程中断，可以在恢复正常后接着采集，而不是从头再来。
- 友好的信息采集。使用适当的延迟/间隔访问目标网站，不给目标网站造成严重的压力。

#### 爬虫配置项

- 目标url
- 选项：深度优先或广度优先
- 配置项：不采集的资源类型列表。根据文件后缀或是HTTP协议标明的资源类型。

#### 数据分析

- 单网页分析，包括不限于
  - 大小
  - 编码
  - 标题
  - 抓取时间
  - 所有的链接地址的域名
- 统计分析
  - 网页数量
  - 网页大小分布
  - 所有链接地址的域名统计



进度表：

起始时间7.31——终止时间8.31

| 时间        | 任务                    | 完成情况 |
| --------- | --------------------- | ---- |
| 7.31-8.1  | 学习Scrapy框架，初步试验爬取网站   | done |
| 8.2-8.4   | 处理网页编码，过滤信息，友好信息采集    | done |
| 8.7-8.9   | 断点恢复，单网页分析            |      |
| 8.10-8.11 | 统计分析网页大小、数量、域名        |      |
| 8.14-8.15 | 理解JavaScript以及动态网站的构成 |      |
| 8.16-8.18 | 使用中目标的代码运行多个网站        |      |
| 8.21-8.23 | 反爬虫措施的处理、代理           |      |
| 8.24-8.25 | 优化、动态添加目标网站           |      |



### 1.Rule的过滤



### 2.单网页数据分析

- 网页大小的提取

  由于腾讯新闻网页中并未包含<u>**'content-length'**</u>元素，所以用如下代码并不能实现：

```python
download_size= int(response.headers['content-length'])
```

​	在scrapy框架中可以用这句话来获取：

```python
download_size = len(response.body)
```

验证：

对于新闻网页：

http://news.qq.com/a/20170706/046701.htm

用上述语句可获得大小：33527

然后找到小目标中获取网页大小的方法验证：(注意修改编码方式为gb2312)

```python
File Exist!
file:中国人的一天：一年后，爬悬崖上学的孩子们还好吗？_新闻_腾讯网.txt
ALL Down
网页标题是：中国人的一天：一年后，爬悬崖上学的孩子们还好吗？_新闻_腾讯网
网页大小是：33527b
```

成功！

- 网页编码

  首先用的是

```python
qqnews['charset'] = response.encoding
```

​	不是很准确，会把gb2312判断成gb18030，用xpath提取属性可以

```python
qqnews['charset'] = response.xpath('/html/head/meta/@charset').extract()[0]#网页编码
```

- 标题

  ```python
  qqnews['title'] = response.xpath('//html/head/title/text()').extract()#标题
  ```


- 抓取时间

  ```python
  #时间中的秒数为整数
  qqnews['date'] = str(datetime.now().replace(microsecond=0))#抓取时间
  ```


- 数据库的连接

  ```python
  conn = pymysql.connect(
  host = 'localhost',  # 连接的是本地数据库
  user = 'root',  # mysql用户名
  passwd = None,  # 密码
  db = 'first',  # 数据库的名字
  charset = 'utf8')# 默认的编码方式：
  #cursorclass = pymysql.cursors.DictCursor)
  cursor = conn.cursor()
  ```

### 3.去重性

set()去重

```python
class CustomURLFilter(object):
      """根据url过滤"""
      def __init__(self):
          self.urls_seen = set()

      def request_seen(self,item,spider):
          if item['url'] in self.urls_seen:
              raise DropItem("Duplicate item found: %s" % item)
          else:
              self.urls_seen.add(item['url'])
              return item
```

实现截图：

  iMon/diary/png/1.PNG

### 4.亟待解决

1.其他链接的统计

2.网页数量太少

3.断点恢复

4.数据库分析

